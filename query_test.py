from langchain_community.vectorstores import FAISS  # âœ… Updated FAISS import
from langchain_huggingface import HuggingFaceEmbeddings  # âœ… Updated embeddings import
from langchain_community.llms import Ollama  # âœ… Use local LLaMA 3
from langchain.chains import RetrievalQA  # âœ… RAG pipeline

# 1ï¸âƒ£ å…è®¸ FAISS ååºåˆ—åŒ–
vector_db = FAISS.load_local(
    "vector_db",
    HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2"),
    allow_dangerous_deserialization=True  # âœ… å…è®¸åŠ è½½ FAISS æ—§æ ¼å¼
)
retriever = vector_db.as_retriever()

# 2ï¸âƒ£ ä½¿ç”¨æœ¬åœ° LLaMA 3ï¼ˆOllama è¿è¡Œï¼‰
llm = Ollama(model="llama3")  # å¯æ¢æˆ "mistral"
qa = RetrievalQA.from_chain_type(llm, retriever=retriever)

# 3ï¸âƒ£ ç»ˆç«¯äº¤äº’æ¨¡å¼
print("ğŸ‰ åŠ¨æ¼«ç™¾ç§‘ AI å·²å¯åŠ¨ï¼è¾“å…¥ 'exit' é€€å‡º")
while True:
    question = input("\nğŸ“¢ è¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š")
    if question.lower() == "exit":
        print("ğŸ‘‹ å†è§ï¼")
        break
    answer = qa.run(question)
    print("ğŸ¤– AI å›ç­”ï¼š", answer)
