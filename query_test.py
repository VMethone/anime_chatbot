from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
import langdetect

# åŠ è½½ FAISS å‘é‡æ•°æ®åº“
vector_db = FAISS.load_local(
    "vector_db",
    HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"),
    allow_dangerous_deserialization=True
)

# åŠ è½½æœ¬åœ° LLaMA 3
llm = Ollama(model="llama3")

while True:
    user_input = input("ğŸ“¢ è¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š")
    if user_input.lower() == "exit":
        break

    # è¯­è¨€æ£€æµ‹
    detected_lang = langdetect.detect(user_input)
    print(f"ğŸ” æ£€æµ‹åˆ°è¯­è¨€: {detected_lang}")

    # æ£€ç´¢ç›¸å…³æ•°æ®
    search_results = vector_db.similarity_search(user_input, k=5)

    # ğŸ›  Debugï¼šæ‰“å°æ£€ç´¢ç»“æœ
    print("\nğŸ” æœç´¢ç»“æœï¼ˆDEBUGï¼‰ï¼š")
    for doc in search_results:
        print(f"[æ¥æº: {doc.metadata.get('æ¥æº', 'æœªçŸ¥')}]\n{doc.page_content}\n")

    context = "\n".join([doc.page_content for doc in search_results])

    if not context.strip():
        print("ğŸ¤– AI å›ç­”: æˆ‘ä¸çŸ¥é“ã€‚")
        continue

    # âœ… ä¿®æ”¹ Promptï¼Œå¼ºåˆ¶è¦æ±‚ AI **ä½¿ç”¨ä¸­æ–‡å›ç­”**
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç²¾é€šåŠ¨æ¼«å’ŒèŒå¨˜ç™¾ç§‘çš„ AIï¼Œä¸“é—¨æä¾›è¯¦ç»†çš„åŠ¨æ¼«è§’è‰²ä¿¡æ¯ã€‚
    **è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”**ï¼Œå¹¶å°½å¯èƒ½è¡¥å……ç»†èŠ‚ï¼Œå¦‚è§’è‰²èƒŒæ™¯ã€æ€§æ ¼ã€ç›¸å…³ä½œå“ç­‰ã€‚

    **èƒŒæ™¯çŸ¥è¯†**ï¼š
    {context}

    **é—®é¢˜**ï¼š
    {user_input}

    **è¯·è¯¦ç»†å›ç­”ï¼Œå¹¶åªåŸºäºèƒŒæ™¯çŸ¥è¯†ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯**ï¼š
    """
    
    answer = llm.invoke(prompt)
    print(f"\nğŸ¤– AI å›ç­”:\n{answer}\n")
