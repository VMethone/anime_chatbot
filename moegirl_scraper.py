import requests
from bs4 import BeautifulSoup
import json
import os
import time
from urllib.parse import quote, urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class MoegirlScraper:
    def __init__(self, base_url="https://moegirl.uk/", data_dir="moegirl_data"):
        self.base_url = base_url
        self.data_dir = data_dir
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Accept-Language": "zh-CN,zh;q=0.9"
        }
        
        # é…ç½®é‡è¯•æœºåˆ¶
        self.session = requests.Session()
        retry = Retry(total=3, backoff_factor=1)
        adapter = HTTPAdapter(max_retries=retry)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)

    def fetch_page(self, title):
        """å¸¦é‡è¯•æœºåˆ¶çš„é¡µé¢è¯·æ±‚"""
        encoded_title = quote(title)
        url = f"{self.base_url}{encoded_title}"
        print(f"ğŸ” æ­£åœ¨è®¿é—®: {url}")

        try:
            response = self.session.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {str(e)}")
            return None

    def parse_article(self, html, url):
        """å®Œæ•´å†…å®¹è§£æå¼•æ“"""
        soup = BeautifulSoup(html, "html.parser")
        
        # è§£æåŸºç¡€ä¿¡æ¯
        article_data = {
            "åç§°": self._get_title(soup),
            "æ¥æº": url,
            "ç« èŠ‚å†…å®¹": [],
            "è§’è‰²ä¿¡æ¯": []
        }

        # è§£æç« èŠ‚ç»“æ„
        content_div = soup.find('div', {'id': 'mw-content-text'})
        if content_div:
            current_section = None
            for element in content_div.find_all(['h2', 'h3', 'p', 'ul', 'ol', 'table', 'div']):
                if element.name in ['h2', 'h3']:
                    if current_section:
                        article_data["ç« èŠ‚å†…å®¹"].append(current_section)
                    current_section = self._create_section(element)
                elif current_section:
                    self._process_element(element, current_section)
            
            if current_section:
                article_data["ç« èŠ‚å†…å®¹"].append(current_section)

        # è§£æè§’è‰²ä¿¡æ¯
        article_data["è§’è‰²ä¿¡æ¯"] = self.extract_characters(soup)

        # æ¸…ç†ç©ºç« èŠ‚
        article_data["ç« èŠ‚å†…å®¹"] = [s for s in article_data["ç« èŠ‚å†…å®¹"] if s["content"] or s["title"] in ["æ³¨é‡Šä¸å¤–éƒ¨é“¾æ¥"]]

        self._save_data(article_data)
        return article_data

    def _create_section(self, element):
        """åˆ›å»ºæ–°ç« èŠ‚"""
        title_span = element.find('span', class_='mw-headline')
        return {
            "level": element.name,
            "title": title_span.get_text(strip=True) if title_span else "æ— æ ‡é¢˜",
            "content": []
        }

    def _get_title(self, soup):
        """è·å–é¡µé¢æ ‡é¢˜"""
        title_tag = soup.find("h1", {"id": "firstHeading"})
        return title_tag.text.strip() if title_tag else "æœªçŸ¥æ ‡é¢˜"

    def _process_element(self, element, section):
        """å¤„ç†å„ç§å†…å®¹å…ƒç´ """
        # è·³è¿‡å¯¼èˆªæ ç­‰æ— å…³å†…å®¹
        if 'navbox' in element.get('class', []):
            return

        # å†…å®¹å¤„ç†å™¨æ˜ å°„
        handlers = {
            'p': self._parse_paragraph,
            'ul': self._parse_list,
            'ol': self._parse_list,
            'table': self._parse_table,
            'div': self._parse_special_div
        }

        if element.name in handlers:
            content = handlers[element.name](element)
            if content:
                section["content"].append(content)

    def _parse_paragraph(self, p):
        """è§£ææ®µè½"""
        text = p.get_text(' ', strip=True)
        text = text.replace('ï¼ˆï¼‰', '').replace('()', '')  # æ¸…ç†ç©ºæ‹¬å·
        if len(text) > 30:  # è¿‡æ»¤çŸ­æ–‡æœ¬
            return {"type": "paragraph", "text": text}
        return None

    def _parse_list(self, ul):
        """è§£æåˆ—è¡¨"""
        items = [li.get_text(' ', strip=True) for li in ul.find_all('li')]
        return {"type": "list", "items": items}

    def _parse_table(self, table):
        """è§£æè¡¨æ ¼æ•°æ®"""
        if 'wikitable' not in table.get('class', []):
            return None

        rows = []
        for tr in table.find_all('tr'):
            cells = []
            for td in tr.find_all(['th', 'td']):
                # æ¸…ç†å•å…ƒæ ¼å†…å®¹
                cell_text = td.get_text(' ', strip=True)
                cell_text = cell_text.replace('\xa0', ' ')  # æ›¿æ¢ä¸é—´æ–­ç©ºæ ¼
                cells.append(cell_text)
            if cells:
                rows.append(cells)

        if not rows:
            return None

        return {
            "type": "table",
            "header": rows[0] if len(rows) > 1 else [],
            "rows": rows[1:] if len(rows) > 1 else rows
        }

    def _parse_special_div(self, div):
        """è§£æç‰¹æ®Šdivå†…å®¹"""
        # å¤„ç†å¸¦å›¾ç‰‡çš„è¯´æ˜æ¡†
        if 'thumb' in div.get('class', []):
            img_tag = div.find('img')
            caption = div.find('div', class_='thumbcaption')
            return {
                "type": "image_box",
                "image": urljoin(self.base_url, img_tag['src']) if img_tag else None,
                "caption": caption.get_text(strip=True) if caption else None
            }
        return None

    def extract_characters(self, soup):
        """è§’è‰²ä¿¡æ¯è§£æ"""
        characters = []
        for card in soup.select('.role-list-item'):
            # è§£æè§’è‰²åç§°
            name_tag = card.select_one('.role-name a')
            if not name_tag:
                continue

            # ä¸­æ—¥æ–‡åç§°å¤„ç†
            ch_name = name_tag.get_text(strip=True)
            jp_name = self._parse_japanese_name(card)
            full_name = f"{ch_name}ï¼ˆ{jp_name}ï¼‰" if jp_name else ch_name

            # æ„å»ºè§’è‰²ä¿¡æ¯
            role_info = {
                "è§’è‰²å": full_name,
                "å›¾ç‰‡": self._parse_character_image(card),
                "è¯¦ç»†ä¿¡æ¯": self._parse_character_details(card)
            }
            characters.append(role_info)

        return characters

    def _parse_japanese_name(self, card):
        """è§£ææ—¥æ–‡åç§°"""
        jp_div = card.select_one('.role-name span[lang="ja"]')
        if not jp_div:
            return None

        name_parts = []
        for ruby in jp_div.find_all('ruby'):
            kanji = ''.join([rb.get_text(strip=True) for rb in ruby.find_all('rb')])
            kana = ruby.find('rt').get_text(strip=True) if ruby.find('rt') else ""
            name_parts.append(f"{kanji}({kana})" if kana else kanji)
        
        return ' '.join(name_parts)

    def _parse_character_image(self, card):
        """è§£æè§’è‰²å›¾ç‰‡"""
        img_tag = card.find('img')
        if img_tag and 'src' in img_tag.attrs:
            return urljoin(self.base_url, img_tag['src'])
        return None

    def _parse_character_details(self, card):
        """è§£æè¯¦ç»†ä¿¡æ¯"""
        details = {}
        info_div = card.select_one('.role-info-inner')
        if not info_div:
            return details

        # è§£æé”®å€¼å¯¹
        current_key = None
        for element in info_div.children:
            if element.name == 'p':
                text = element.get_text(' ', strip=True)
                if 'ï¼š' in text:
                    key, val = text.split('ï¼š', 1)
                    details[key.strip()] = val.strip()
                    current_key = key.strip()
                elif current_key:
                    details[current_key] += " " + text.strip()
            
            # è§£æè¡¥å……è¯´æ˜
            elif element.name == 'dl':
                details.setdefault('è¡¥å……è¯´æ˜', []).extend(
                    [dd.get_text(' ', strip=True) for dd in element.find_all('dd')]
                )

        return details

    def _save_data(self, data):
        """ä¿å­˜æ•°æ®"""
        safe_title = ''.join(c for c in data["åç§°"] if c.isalnum() or c in ' _-')
        filename = os.path.join(self.data_dir, f"{safe_title}.json")
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {filename}")

    def crawl_article(self, title):
        """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
        html = self.fetch_page(title)
        if not html:
            return None
        return self.parse_article(html, f"{self.base_url}{quote(title)}")

if __name__ == "__main__":
    scraper = MoegirlScraper()
    name = input("ğŸ“¢ è¯·è¾“å…¥è¦çˆ¬å–çš„æ¡ç›®åç§°: ")
    
    start_time = time.time()
    result = scraper.crawl_article(name)
    
    if result:
        print(f"â±ï¸ çˆ¬å–å®Œæˆï¼Œè€—æ—¶ {time.time()-start_time:.2f}ç§’")
        print(f"ğŸ·ï¸ å‘ç° {len(result['ç« èŠ‚å†…å®¹'])} ä¸ªç« èŠ‚")
        print(f"ğŸ‘¥ è§£æåˆ° {len(result['è§’è‰²ä¿¡æ¯'])} ä¸ªè§’è‰²ä¿¡æ¯")