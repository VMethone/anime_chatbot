import os
from langchain_community.vectorstores import FAISS  # FAISS moved to langchain_community
from langchain_community.document_loaders import TextLoader  # Use updated TextLoader
from langchain_huggingface import HuggingFaceEmbeddings  # Use langchain_huggingface
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Text splitter

# è¯»å–åŠ¨æ¼«ç™¾ç§‘æ•°æ®
DATA_PATH = "data/anime_encyclopedia.txt"
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError("âŒ è¯·æä¾› data/anime_encyclopedia.txt æ–‡ä»¶")

loader = TextLoader(DATA_PATH)
documents = loader.load()

# åˆ‡åˆ†æ–‡æœ¬
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
texts = text_splitter.split_documents(documents)

# Debugging Step: Print the number of text chunks
print(f"ğŸ“œ Total Chunks Created: {len(texts)}")

# Check if text chunks are empty
if len(texts) == 0:
    raise ValueError("âŒ No text chunks were created! Please check your data file.")

# ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼ˆæ›¿æ¢ OpenAIï¼‰
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ç”Ÿæˆ FAISS å‘é‡æ•°æ®åº“
vector_db = FAISS.from_documents(texts, embedding_model)

# ä¿å­˜å‘é‡æ•°æ®åº“
os.makedirs("vector_db", exist_ok=True)
vector_db.save_local("vector_db")

print("âœ… åŠ¨æ¼«ç™¾ç§‘å‘é‡æ•°æ®åº“å·²æˆåŠŸåˆ›å»ºï¼")
