from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.llms import Ollama  # âœ… æœ¬åœ° LLaMA 3
import langdetect  # âœ… è¯­è¨€æ£€æµ‹

# âœ… å…è®¸ FAISS ååºåˆ—åŒ–ï¼ˆç¡®ä¿æ•°æ®åº“æ–‡ä»¶æ˜¯è‡ªå·±ç”Ÿæˆçš„ï¼‰
vector_db = FAISS.load_local(
    "vector_db",
    HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"),
    allow_dangerous_deserialization=True
)

# âœ… åŠ è½½æœ¬åœ° LLaMA 3
llm = Ollama(model="llama3")  # ç¡®ä¿ä½ å·²ä¸‹è½½ `ollama pull llama3`

while True:
    user_input = input("ğŸ“¢ è¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š")
    if user_input.lower() == "exit":
        break

    # âœ… æ£€æµ‹è¾“å…¥è¯­è¨€
    detected_lang = langdetect.detect(user_input)
    print(f"ğŸ” æ£€æµ‹åˆ°è¯­è¨€: {detected_lang}")

    # âœ… è¿›è¡Œ FAISS ç›¸ä¼¼æ€§æœç´¢ï¼ˆè°ƒæ•´ k=5 è·å–æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
    search_results = vector_db.similarity_search(user_input, k=5)
    context = "\n".join([doc.page_content for doc in search_results])

    # âœ… å¦‚æœ `FAISS` æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œç›´æ¥å›ç­” "æˆ‘ä¸çŸ¥é“"
    if not context.strip():
        print("ğŸ¤– AI å›ç­”: æˆ‘ä¸çŸ¥é“ã€‚")
        continue

    # âœ… è®© AI åªåŸºäº FAISS ç»“æœå›ç­”ï¼Œå¹¶æ˜ç¡®è¦æ±‚ **ä¸èƒ½ç¼–é€ ç­”æ¡ˆ**
    if detected_lang.startswith("zh"):
        prompt = f"""è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œå¹¶ä¸”åªèƒ½åŸºäºæä¾›çš„èƒŒæ™¯çŸ¥è¯†ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¯·å›ç­” "æˆ‘ä¸çŸ¥é“"ã€‚

èƒŒæ™¯çŸ¥è¯†ï¼š
{context}

é—®é¢˜ï¼š{user_input}

è¯·ç”¨ä¸­æ–‡ç®€æ´å›ç­”ï¼š
"""
    else:
        prompt = f"""Please answer the following question in English.
You must only use the given context. Do not make up any information.
If the answer is not found in the context, respond with "I don't know."

Context:
{context}

Question: {user_input}

Provide a concise answer in English:
"""

    answer = llm.invoke(prompt)

    print(f"ğŸ¤– AI å›ç­”:\n{answer}")