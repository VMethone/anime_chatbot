import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import quote, urljoin
import re
import time
import random

BASE_URL = "https://zh.moegirl.org.cn"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Referer": BASE_URL + "/Mainpage",
    "DNT": "1"
}

def clean_text(text):
    """å¢å¼ºç‰ˆæ–‡æœ¬æ¸…ç†"""
    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)  # ç§»é™¤æ³¨é‡Š
    text = re.sub(r'\[\d+\]', '', text)  # ç§»é™¤å¼•ç”¨æ ‡è®°
    text = re.sub(r'\s+', ' ', text).strip()
    return text.replace('\u200b', '').replace('\xa0', ' ')

def parse_cell_content(cell):
    """æ·±åº¦è§£æå•å…ƒæ ¼å†…å®¹"""
    content_parts = []
    
    # å¤„ç†ç‰¹æ®Šå…ƒç´ 
    for element in cell.descendants:
        if isinstance(element, str):
            content_parts.append(element.strip())
        elif element.name == 'img':
            alt = element.get('alt', 'å›¾ç‰‡').strip()
            src = urljoin(BASE_URL, element.get('src', ''))
            content_parts.append(f"[å›¾ç‰‡: {alt}]({src})")
        elif element.name == 'a' and element.get('href'):
            text = element.text.strip()
            link = urljoin(BASE_URL, element['href'])
            content_parts.append(f"{text}ï¼ˆ{link}ï¼‰")
        elif element.name == 'br':
            content_parts.append('\n')
        elif 'heimu' in element.get('class', []):  # å¤„ç†é»‘å¹•æ–‡æœ¬
            content_parts.append(f"||{element.text.strip()}||")
    
    # åˆå¹¶å¤„ç†ç»“æœ
    raw_text = ''.join(content_parts)
    return clean_text(raw_text)

def parse_infobox(infobox):
    """å®Œæ•´Infoboxè§£æå®ç°"""
    anime_data = {}
    current_headers = []
    
    try:
        # æ’é™¤åµŒå¥—è¡¨æ ¼
        if infobox.find_parent('table'):
            return {}
            
        for row in infobox.find_all('tr'):
            # è§£æè¡¨å¤´è¡Œ
            th_list = row.find_all('th')
            if th_list:
                current_headers = []
                for th in th_list:
                    header = clean_text(th.get_text(separator=" "))
                    rowspan = int(th.get('rowspan', 1))
                    current_headers.extend([header] * rowspan)
                continue
                
            # è§£ææ•°æ®è¡Œ
            td_list = row.find_all('td')
            if not td_list:
                continue
                
            # å¤„ç†è·¨åˆ—å•å…ƒæ ¼
            if len(td_list) == 1 and current_headers:
                anime_data[current_headers[0]] = parse_cell_content(td_list[0])
                continue
                
            # å¸¸è§„é”®å€¼å¯¹å¤„ç†
            for header, td in zip(current_headers, td_list):
                anime_data[header] = parse_cell_content(td)
                
    except Exception as e:
        print(f"âš ï¸ Infoboxè§£æå¼‚å¸¸: {str(e)}")
        
    return anime_data

def get_anime_info(anime_name):
    """å®Œæ•´ä¿¡æ¯è·å–æµç¨‹"""
    session = requests.Session()
    encoded_name = quote(anime_name.strip().replace(' ', '_'), safe='')
    url = f"{BASE_URL}/{encoded_name}"
    
    try:
        # éšæœºå»¶è¿Ÿé˜²æ­¢å°ç¦
        time.sleep(random.uniform(1, 2))
        
        response = session.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # å¼ºåˆ¶ç¼–ç å¤„ç†
        if response.encoding.lower() != 'utf-8':
            response.encoding = 'utf-8'
            
        soup = BeautifulSoup(response.text, 'lxml')
        
        # æ–°ç‰ˆInfoboxæ£€æµ‹
        infobox = soup.find('table', class_=re.compile(r'wikitable|infobox'))
        
        anime_data = {
            "åç§°": anime_name,
            "æ¥æº": url,
            "åŸºæœ¬ä¿¡æ¯": {},
            "å‰§æƒ…ç®€ä»‹": ""
        }
        
        if infobox:
            anime_data["åŸºæœ¬ä¿¡æ¯"] = parse_infobox(infobox)
            
        # å¢å¼ºç‰ˆç®€ä»‹æå–
        content_div = soup.find('div', class_='mw-parser-output')
        if content_div:
            for element in content_div.children:
                if element.name == 'p':
                    text = clean_text(element.get_text())
                    if len(text) > 100 and not re.search(r'^æœ¬æ¡ç›®éœ€è¦', text):
                        anime_data["å‰§æƒ…ç®€ä»‹"] = text
                        break
                elif element.name in ['h2', 'div', 'table']:
                    break
                    
        # æ•°æ®æ¸…æ´—
        anime_data["åŸºæœ¬ä¿¡æ¯"] = {
            k: v for k, v in anime_data["åŸºæœ¬ä¿¡æ¯"].items() 
            if v and k not in [''] and len(v) < 500
        }
        
        return anime_data
        
    except Exception as e:
        print(f"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}")
        return None

if __name__ == "__main__":
    anime_name = input("ğŸ“¢ è¯·è¾“å…¥åŠ¨ç”»åç§°: ").strip()
    if not anime_name:
        print("âš ï¸ è¾“å…¥ä¸èƒ½ä¸ºç©º")
        exit()
        
    result = get_anime_info(anime_name)
    
    if result:
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print("âŒ æœªèƒ½è·å–æœ‰æ•ˆä¿¡æ¯")